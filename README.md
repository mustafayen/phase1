# Phase 1 - Automatic Code and Test Generation

This repository contains the results of Phase 1 of the **BLG475E Software Engineering** course project.

The project focuses on using Large Language Models (LLMs) to **automatically generate Python functions and unit tests** based on programming problems from the HumanEval dataset.

---

## Project Structure

```
outputsGPT/
├── gpt3.5/
│   ├── task_0_code.py
│   ├── task_0_test.py
│   ├── task_1_code.py
│   ├── task_1_test.py
│   └── ...
outputsGemini/
├── gemini2.0/
│   ├── task_0_code.py
│   ├── task_0_test.py
│   ├── task_1_code.py
│   ├── task_1_test.py
│   └── ...
run_all_tests_gemini.py
run_all_tests_gpt.py
humaneval_prompts_with_difficulty.csv
generate_gpt35_tests.py
run_coverageGemini.py
run_coverageGPT.py
```

- `outputsGPT/gpt3.5/`: Solutions generated by GPT-3.5-turbo.
- `outputsGemini/gemini2.0`: Solutions generated by Gemini 2.0 Flash.
- `humaneval_prompts_with_difficulty.csv`: Contains the HumanEval problem prompts along with their associated difficulty levels, used for selecting and organizing tasks.
- `generate_gpt35_tests.py`: Used GPT-3.5 Turbo to automatically generate Python code and corresponding unit test files based on problem descriptions.
- `run_all_tests_gemini.py`: Script to execute and evaluate all test cases.
- `run_all_tests_gpt.py`: Script to execute and evaluate coverage of all test cases for GPT3.5.
- `run_coverageGemini.py`: Script to execute and evaluate coverage of all test cases for Gemini2.0.
- `run_coverageGPT.py`: Script to execute and evaluate all test cases.


---

## Models Used

| Model              | Purpose                    |
| ------------------ | -------------------------- |
| GPT-3.5-turbo       | Initial code generation     |
| Gemini 2.0 Flash    | Additional robust test generation |

---

## Results from GPT 3.5

| Category          | Value  |
|-------------------|--------|
| Total Tests Run   | 119    |
| Successful Tests  | 96     |
| Failed Tests      | 23     |
| Test Success Rate | 80.67% |


---

## Results from Gemini 2.0

| Category          | Value  |
|-------------------|--------|
| Total Tests Run   | 144    |
| Successful Tests  | 140    |
| Failed Tests      | 4      |
| Test Success Rate | 97.22% |


---


## Coverage Results

We used the `run_coverageGemini.py` and `run_coverageGPT.py` tools to measure line-by-line coverage of the generated tests.

### GPT-3.5 Turbo Coverage

| File Count | Total Statements | Missed Statements | Coverage |
|------------|------------------|-------------------|----------|
| 30 tasks   | 759              | 6                 | 99%      |

Most functions were fully covered by the generated tests. A few minor uncovered branches exist due to insufficient edge cases.

### Gemini 2.0 Flash Coverage

| File Count | Total Statements | Missed Statements | Coverage |
|------------|------------------|-------------------|----------|
| 30 tasks   | 783              | 7                 | 99%      |

Gemini-generated tests achieved nearly full coverage across all files. Only a few statements were missed due to edge-case assumptions.


---

## Dataset

- HumanEval Dataset by OpenAI
- Available at:
  - [GitHub Repository](https://github.com/openai/human-eval)

---

## Repository

- GitHub: [https://github.com/mustafayen/phase1](https://github.com/mustafayen/phase1)

---

## Author

- Mustafa Yavuz Engin  
- Student ID: 150200708

- Çiğdem Onur  
- Student ID: 150190022

- Course: BLG475E